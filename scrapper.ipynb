{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: tqdm in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (4.65.0)\n",
      "Requirement already satisfied: sqlalchemy in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (2.0.19)\n",
      "Requirement already satisfied: tabula-py in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (2.7.0)\n",
      "Requirement already satisfied: typing-extensions>=4.2.0 in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (from sqlalchemy) (4.7.1)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (from sqlalchemy) (2.0.2)\n",
      "Requirement already satisfied: pandas>=0.25.3 in /home/joaomarcostg/.local/lib/python3.10/site-packages (from tabula-py) (2.0.1)\n",
      "Requirement already satisfied: numpy in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (from tabula-py) (1.25.1)\n",
      "Requirement already satisfied: distro in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (from tabula-py) (1.8.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /home/joaomarcostg/.local/lib/python3.10/site-packages (from pandas>=0.25.3->tabula-py) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in /home/joaomarcostg/.local/lib/python3.10/site-packages (from pandas>=0.25.3->tabula-py) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /home/joaomarcostg/miniconda3/envs/scrapper/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas>=0.25.3->tabula-py) (1.16.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import time\n",
    "import requests\n",
    "import json\n",
    "import csv\n",
    "import pandas as pd\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.service import Service\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from urllib.parse import urlparse\n",
    "from bs4 import BeautifulSoup \n",
    "from datetime import datetime\n",
    "\n",
    "%pip install tqdm sqlalchemy tabula-py\n",
    "\n",
    "\n",
    "# Create the \"tables\" folder if it doesn't exist\n",
    "tables_folder_path = \"./tables\"\n",
    "if not os.path.exists(tables_folder_path):\n",
    "    os.makedirs(tables_folder_path)\n",
    "\n",
    "\n",
    "# Create the \"matrizes\" folder if it doesn't exist\n",
    "pdfs_folder_path = \"./courses_pdfs\"\n",
    "if not os.path.exists(pdfs_folder_path):\n",
    "    os.makedirs(pdfs_folder_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "from sqlalchemy import create_engine\n",
    "\n",
    "# Replace 'your_db_name', 'your_username', and 'your_password' with your PostgreSQL credentials\n",
    "db_name = 'gradeufop_db'\n",
    "username = 'postgres'\n",
    "password = '12345678'\n",
    "\n",
    "# Replace 'localhost' with the appropriate host if your PostgreSQL server is running on a different machine\n",
    "host = 'localhost'\n",
    "port = '5432'\n",
    "\n",
    "# Create a connection to the PostgreSQL database\n",
    "conn_str = f\"postgresql://{username}:{password}@{host}:{port}/{db_name}\"\n",
    "engine = create_engine(conn_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "def format_course_name(text):\n",
    "    # Remove accent marks\n",
    "    text = ''.join(c for c in unicodedata.normalize('NFD', text) if unicodedata.category(c) != 'Mn')\n",
    "    \n",
    "    # Replace symbols with a hyphen\n",
    "    text = re.sub(r'[^a-zA-Z0-9]+', '-', text)\n",
    "    \n",
    "    # Remove leading and trailing hyphens\n",
    "    text = text.strip('-')\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Busca cursos e salva os .pdfs na pasta /matrizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando os links .pdf ...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Progresso:  16%|#5        | 7/44 [00:09<00:48,  1.31s/it, curso=CiÃªncia e Tecnologia de Alimentos]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 47\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[39mwith\u001b[39;00m tqdm(total\u001b[39m=\u001b[39m\u001b[39mlen\u001b[39m(links), desc\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mProgresso\u001b[39m\u001b[39m\"\u001b[39m, ascii\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m) \u001b[39mas\u001b[39;00m pbar:\n\u001b[1;32m     46\u001b[0m     \u001b[39mfor\u001b[39;00m i, link \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(links):\n\u001b[0;32m---> 47\u001b[0m         driver\u001b[39m.\u001b[39;49mget(link)\n\u001b[1;32m     49\u001b[0m         matriz_elements \u001b[39m=\u001b[39m driver\u001b[39m.\u001b[39mfind_elements(\n\u001b[1;32m     50\u001b[0m             By\u001b[39m.\u001b[39mCLASS_NAME, \u001b[39m\"\u001b[39m\u001b[39mfield-name-field-matriz-curricular\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m     51\u001b[0m         )\n\u001b[1;32m     53\u001b[0m         \u001b[39mfor\u001b[39;00m element \u001b[39min\u001b[39;00m matriz_elements:\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:449\u001b[0m, in \u001b[0;36mWebDriver.get\u001b[0;34m(self, url)\u001b[0m\n\u001b[1;32m    447\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget\u001b[39m(\u001b[39mself\u001b[39m, url: \u001b[39mstr\u001b[39m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    448\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads a web page in the current browser session.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 449\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mexecute(Command\u001b[39m.\u001b[39;49mGET, {\u001b[39m\"\u001b[39;49m\u001b[39murl\u001b[39;49m\u001b[39m\"\u001b[39;49m: url})\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/webdriver.py:438\u001b[0m, in \u001b[0;36mWebDriver.execute\u001b[0;34m(self, driver_command, params)\u001b[0m\n\u001b[1;32m    435\u001b[0m     \u001b[39melif\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m params:\n\u001b[1;32m    436\u001b[0m         params[\u001b[39m\"\u001b[39m\u001b[39msessionId\u001b[39m\u001b[39m\"\u001b[39m] \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msession_id\n\u001b[0;32m--> 438\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcommand_executor\u001b[39m.\u001b[39;49mexecute(driver_command, params)\n\u001b[1;32m    439\u001b[0m \u001b[39mif\u001b[39;00m response:\n\u001b[1;32m    440\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39merror_handler\u001b[39m.\u001b[39mcheck_response(response)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:290\u001b[0m, in \u001b[0;36mRemoteConnection.execute\u001b[0;34m(self, command, params)\u001b[0m\n\u001b[1;32m    288\u001b[0m data \u001b[39m=\u001b[39m utils\u001b[39m.\u001b[39mdump_json(params)\n\u001b[1;32m    289\u001b[0m url \u001b[39m=\u001b[39m \u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m{\u001b[39;00m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_url\u001b[39m}\u001b[39;00m\u001b[39m{\u001b[39;00mpath\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m\n\u001b[0;32m--> 290\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_request(command_info[\u001b[39m0\u001b[39;49m], url, body\u001b[39m=\u001b[39;49mdata)\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/selenium/webdriver/remote/remote_connection.py:311\u001b[0m, in \u001b[0;36mRemoteConnection._request\u001b[0;34m(self, method, url, body)\u001b[0m\n\u001b[1;32m    308\u001b[0m     body \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    310\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mkeep_alive:\n\u001b[0;32m--> 311\u001b[0m     response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_conn\u001b[39m.\u001b[39;49mrequest(method, url, body\u001b[39m=\u001b[39;49mbody, headers\u001b[39m=\u001b[39;49mheaders)\n\u001b[1;32m    312\u001b[0m     statuscode \u001b[39m=\u001b[39m response\u001b[39m.\u001b[39mstatus\n\u001b[1;32m    313\u001b[0m \u001b[39melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/site-packages/urllib3/_request_methods.py:118\u001b[0m, in \u001b[0;36mRequestMethods.request\u001b[0;34m(self, method, url, body, fields, headers, json, **urlopen_kw)\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mrequest_encode_url(\n\u001b[1;32m    111\u001b[0m         method,\n\u001b[1;32m    112\u001b[0m         url,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[39m*\u001b[39m\u001b[39m*\u001b[39murlopen_kw,\n\u001b[1;32m    116\u001b[0m     )\n\u001b[1;32m    117\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 118\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mrequest_encode_body(\n\u001b[1;32m    119\u001b[0m         method, url, fields\u001b[39m=\u001b[39;49mfields, headers\u001b[39m=\u001b[39;49mheaders, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49murlopen_kw\n\u001b[1;32m    120\u001b[0m     )\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/site-packages/urllib3/_request_methods.py:217\u001b[0m, in \u001b[0;36mRequestMethods.request_encode_body\u001b[0;34m(self, method, url, fields, headers, encode_multipart, multipart_boundary, **urlopen_kw)\u001b[0m\n\u001b[1;32m    213\u001b[0m     extra_kw[\u001b[39m\"\u001b[39m\u001b[39mheaders\u001b[39m\u001b[39m\"\u001b[39m]\u001b[39m.\u001b[39msetdefault(\u001b[39m\"\u001b[39m\u001b[39mContent-Type\u001b[39m\u001b[39m\"\u001b[39m, content_type)\n\u001b[1;32m    215\u001b[0m extra_kw\u001b[39m.\u001b[39mupdate(urlopen_kw)\n\u001b[0;32m--> 217\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49murlopen(method, url, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mextra_kw)\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/site-packages/urllib3/poolmanager.py:443\u001b[0m, in \u001b[0;36mPoolManager.urlopen\u001b[0;34m(self, method, url, redirect, **kw)\u001b[0m\n\u001b[1;32m    441\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39murlopen(method, url, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkw)\n\u001b[1;32m    442\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m--> 443\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49murlopen(method, u\u001b[39m.\u001b[39;49mrequest_uri, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkw)\n\u001b[1;32m    445\u001b[0m redirect_location \u001b[39m=\u001b[39m redirect \u001b[39mand\u001b[39;00m response\u001b[39m.\u001b[39mget_redirect_location()\n\u001b[1;32m    446\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m redirect_location:\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/site-packages/urllib3/connectionpool.py:790\u001b[0m, in \u001b[0;36mHTTPConnectionPool.urlopen\u001b[0;34m(self, method, url, body, headers, retries, redirect, assert_same_host, timeout, pool_timeout, release_conn, chunked, body_pos, preload_content, decode_content, **response_kw)\u001b[0m\n\u001b[1;32m    787\u001b[0m response_conn \u001b[39m=\u001b[39m conn \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m release_conn \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    789\u001b[0m \u001b[39m# Make the request on the HTTPConnection object\u001b[39;00m\n\u001b[0;32m--> 790\u001b[0m response \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_make_request(\n\u001b[1;32m    791\u001b[0m     conn,\n\u001b[1;32m    792\u001b[0m     method,\n\u001b[1;32m    793\u001b[0m     url,\n\u001b[1;32m    794\u001b[0m     timeout\u001b[39m=\u001b[39;49mtimeout_obj,\n\u001b[1;32m    795\u001b[0m     body\u001b[39m=\u001b[39;49mbody,\n\u001b[1;32m    796\u001b[0m     headers\u001b[39m=\u001b[39;49mheaders,\n\u001b[1;32m    797\u001b[0m     chunked\u001b[39m=\u001b[39;49mchunked,\n\u001b[1;32m    798\u001b[0m     retries\u001b[39m=\u001b[39;49mretries,\n\u001b[1;32m    799\u001b[0m     response_conn\u001b[39m=\u001b[39;49mresponse_conn,\n\u001b[1;32m    800\u001b[0m     preload_content\u001b[39m=\u001b[39;49mpreload_content,\n\u001b[1;32m    801\u001b[0m     decode_content\u001b[39m=\u001b[39;49mdecode_content,\n\u001b[1;32m    802\u001b[0m     \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mresponse_kw,\n\u001b[1;32m    803\u001b[0m )\n\u001b[1;32m    805\u001b[0m \u001b[39m# Everything went great!\u001b[39;00m\n\u001b[1;32m    806\u001b[0m clean_exit \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/site-packages/urllib3/connectionpool.py:536\u001b[0m, in \u001b[0;36mHTTPConnectionPool._make_request\u001b[0;34m(self, conn, method, url, body, headers, retries, timeout, chunked, response_conn, preload_content, decode_content, enforce_content_length)\u001b[0m\n\u001b[1;32m    534\u001b[0m \u001b[39m# Receive the response from the server\u001b[39;00m\n\u001b[1;32m    535\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 536\u001b[0m     response \u001b[39m=\u001b[39m conn\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    537\u001b[0m \u001b[39mexcept\u001b[39;00m (BaseSSLError, \u001b[39mOSError\u001b[39;00m) \u001b[39mas\u001b[39;00m e:\n\u001b[1;32m    538\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_raise_timeout(err\u001b[39m=\u001b[39me, url\u001b[39m=\u001b[39murl, timeout_value\u001b[39m=\u001b[39mread_timeout)\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/site-packages/urllib3/connection.py:461\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    458\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39m.\u001b[39;00m\u001b[39mresponse\u001b[39;00m \u001b[39mimport\u001b[39;00m HTTPResponse\n\u001b[1;32m    460\u001b[0m \u001b[39m# Get the response from http.client.HTTPConnection\u001b[39;00m\n\u001b[0;32m--> 461\u001b[0m httplib_response \u001b[39m=\u001b[39m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mgetresponse()\n\u001b[1;32m    463\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m    464\u001b[0m     assert_header_parsing(httplib_response\u001b[39m.\u001b[39mmsg)\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/http/client.py:1375\u001b[0m, in \u001b[0;36mHTTPConnection.getresponse\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1373\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[1;32m   1374\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1375\u001b[0m         response\u001b[39m.\u001b[39;49mbegin()\n\u001b[1;32m   1376\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mConnectionError\u001b[39;00m:\n\u001b[1;32m   1377\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/http/client.py:318\u001b[0m, in \u001b[0;36mHTTPResponse.begin\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    316\u001b[0m \u001b[39m# read until we get a non-100 response\u001b[39;00m\n\u001b[1;32m    317\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[0;32m--> 318\u001b[0m     version, status, reason \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_read_status()\n\u001b[1;32m    319\u001b[0m     \u001b[39mif\u001b[39;00m status \u001b[39m!=\u001b[39m CONTINUE:\n\u001b[1;32m    320\u001b[0m         \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/http/client.py:279\u001b[0m, in \u001b[0;36mHTTPResponse._read_status\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    278\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_read_status\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[0;32m--> 279\u001b[0m     line \u001b[39m=\u001b[39m \u001b[39mstr\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfp\u001b[39m.\u001b[39;49mreadline(_MAXLINE \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m), \u001b[39m\"\u001b[39m\u001b[39miso-8859-1\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[1;32m    280\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(line) \u001b[39m>\u001b[39m _MAXLINE:\n\u001b[1;32m    281\u001b[0m         \u001b[39mraise\u001b[39;00m LineTooLong(\u001b[39m\"\u001b[39m\u001b[39mstatus line\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/scrapper/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_sock\u001b[39m.\u001b[39;49mrecv_into(b)\n\u001b[1;32m    706\u001b[0m     \u001b[39mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_timeout_occurred \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "# Set up Chrome options\n",
    "chrome_options = Options()\n",
    "chrome_options.add_argument(\"--headless\")  # Run Chrome in headless mode\n",
    "chrome_options.add_argument(\"--disable-gpu\")\n",
    "chrome_options.add_argument(\"--no-sandbox\")\n",
    "\n",
    "# Set up Chrome driver service\n",
    "chromedriver_path = (\n",
    "    \"./chromedriver\"  # Replace with the path to your chromedriver executable\n",
    ")\n",
    "service = Service(chromedriver_path)\n",
    "\n",
    "# Set up Chrome driver\n",
    "driver = webdriver.Chrome(service=service, options=chrome_options)\n",
    "\n",
    "# Navigate to the URL\n",
    "url = \"https://www.escolha.ufop.br/cursos\"\n",
    "driver.get(url)\n",
    "\n",
    "# Wait for the page to load\n",
    "time.sleep(2)\n",
    "\n",
    "# Find elements with class \"ufop-glossary-row\"\n",
    "elements = driver.find_elements(By.CLASS_NAME, \"ufop-glossary-row\")\n",
    "\n",
    "# Extract the href links from child anchor 'a' tags\n",
    "links = []\n",
    "courses_dict = {\"id\": [], \"name\": []}\n",
    "for element in elements:\n",
    "    link_element = element.find_element(By.TAG_NAME, \"a\")\n",
    "    href = link_element.get_attribute(\"href\")\n",
    "    links.append(href)\n",
    "\n",
    "    courses_dict[\"name\"].append(link_element.text)\n",
    "    courses_dict[\"id\"].append(format_course_name(link_element.text))\n",
    "\n",
    "\n",
    "indexes_to_delete = []\n",
    "course_pdfs = []\n",
    "\n",
    "# Navigate to each link and download PDF files\n",
    "print(\"Buscando os links .pdf ...\")\n",
    "with tqdm(total=len(links), desc=\"Progresso\", ascii=True) as pbar:\n",
    "    for i, link in enumerate(links):\n",
    "        driver.get(link)\n",
    "\n",
    "        matriz_elements = driver.find_elements(\n",
    "            By.CLASS_NAME, \"field-name-field-matriz-curricular\"\n",
    "        )\n",
    "\n",
    "        for element in matriz_elements:\n",
    "            link_elements = element.find_elements(By.TAG_NAME, \"a\")\n",
    "\n",
    "            if len(link_elements) == 1:\n",
    "                href = link_elements[0].get_attribute(\"href\")\n",
    "                course_pdfs.append({\"course\": courses_dict[\"id\"][i], \"link\": href})\n",
    "                continue\n",
    "\n",
    "            for link_element in link_elements:\n",
    "                href = link_element.get_attribute(\"href\")\n",
    "                course_type = link_element.text\n",
    "                course_name = f\"{courses_dict['name'][i]} ({course_type})\"\n",
    "                course_id = format_course_name(course_name)\n",
    "                courses_dict[\"name\"].append(course_name)\n",
    "                courses_dict[\"id\"].append(format_course_name(course_name))\n",
    "\n",
    "                course_pdfs.append({\"course\": course_id, \"link\": href})\n",
    "            indexes_to_delete.append(i)\n",
    "\n",
    "        pbar.set_postfix(curso=f\"{courses_dict['name'][i]}\")\n",
    "        pbar.update(1)\n",
    "\n",
    "courses_dict[\"id\"] = [\n",
    "    item for i, item in enumerate(courses_dict[\"id\"]) if i not in indexes_to_delete\n",
    "]\n",
    "courses_dict[\"name\"] = [\n",
    "    item for i, item in enumerate(courses_dict[\"name\"]) if i not in indexes_to_delete\n",
    "]\n",
    "\n",
    "valid_pdf_substrings = [\".pdf\", \"codCurso=\"]\n",
    "for pdf in course_pdfs:\n",
    "    import os\n",
    "\n",
    "for pdf in course_pdfs:\n",
    "    if any(text in pdf[\"link\"] for text in valid_pdf_substrings):\n",
    "        response = requests.get(pdf[\"link\"])\n",
    "        parsed_url = urlparse(pdf[\"link\"])\n",
    "        filename = f\"{pdf['course']}.pdf\"\n",
    "        file_path = os.path.join(pdfs_folder_path, filename)\n",
    "\n",
    "        # Check if the file already exists in the folder\n",
    "        if not os.path.exists(file_path):\n",
    "            with open(file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"{filename} salvo com sucesso.\")\n",
    "        else:\n",
    "            print(f\"{filename} jÃ¡ existe no diretÃ³rio. Pulando...\")\n",
    "\n",
    "\n",
    "# Quit the driver\n",
    "driver.quit()\n",
    "\n",
    "courses_df = pd.DataFrame(courses_dict).sort_values(by='id', ascending = True)\n",
    "courses_df['created_at'] = datetime.now()\n",
    "courses_df.to_csv(f\"{tables_folder_path}/course.csv\", index=False)\n",
    "\n",
    "# Save courses table in database\n",
    "courses_df.to_sql('course', engine, index=False, if_exists='replace')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando DECEA\n"
     ]
    }
   ],
   "source": [
    "import uuid\n",
    "\n",
    "URL = \"https://zeppelin10.ufop.br/HorarioAulas/\"\n",
    "\n",
    "desired_departments = [\"DECSI\", \"DECEA\", \"DEELT\", \"DEENP\", \"DEETE\"]\n",
    "semester = \"23.1\"\n",
    "\n",
    "\n",
    "def get_HTML_content(URL, department):\n",
    "    chrome_options = Options()\n",
    "    chrome_options.add_argument(\"--headless\")\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "    driver.get(URL)\n",
    "    elem = driver.find_element(By.XPATH, \"//*[text()='{}']\".format(department))\n",
    "    elem.click()\n",
    "    URL = driver.current_url\n",
    "    html_source = driver.page_source\n",
    "    soup = BeautifulSoup(html_source, \"lxml\")\n",
    "    driver.quit()\n",
    "    return soup\n",
    "\n",
    "\n",
    "def parse_schedule_string(schedule_string):\n",
    "    entries = []\n",
    "    \n",
    "    if schedule_string == '':\n",
    "        return entries\n",
    "    \n",
    "    schedule_parts = schedule_string.split(\" / \")\n",
    "    \n",
    "    if len(schedule_parts) == 0:\n",
    "        schedule_parts.append(schedule_string)\n",
    "\n",
    "    for part in schedule_parts:\n",
    "        day, time_info = part.split(\" \")\n",
    "        start_time, end_time = time_info.split(\"-\")\n",
    "        class_type = end_time[-2]  # T for theoretical, P for practical\n",
    "        end_time = end_time[:-3]  # Remove the class type from end_time\n",
    "\n",
    "        entry = {\n",
    "            \"day_of_week\": day,\n",
    "            \"start_time\": start_time,\n",
    "            \"end_time\": end_time,\n",
    "            \"class_type\": class_type,\n",
    "        }\n",
    "        entries.append(entry)\n",
    "\n",
    "    return entries\n",
    "\n",
    "\n",
    "def get_field_list(html_content, field):\n",
    "    field_list = []\n",
    "    table = html_content.find(\"table\", {\"id\": \"formPrincipal:tabela\"})\n",
    "    if table:\n",
    "        tbody = table.find(\"tbody\")\n",
    "        tr_elements = tbody.find_all(\"tr\")\n",
    "\n",
    "        for i, tr in enumerate(tr_elements):\n",
    "            if field == \"descricao\":\n",
    "                span = tr.find(\n",
    "                    \"span\", {\"id\": \"formPrincipal:tabela:{}:{}\".format(i, \"disciplina\")}\n",
    "                )\n",
    "                title = span.find_parent(\"a\").get(\n",
    "                    \"title\"\n",
    "                )  # Extract the 'title' attribute of the parent <a> tag\n",
    "                field_list.append(title)\n",
    "                continue\n",
    "\n",
    "            span = tr.find(\n",
    "                \"span\", {\"id\": \"formPrincipal:tabela:{}:{}\".format(i, field)}\n",
    "            )\n",
    "            field_list.append(span.text)\n",
    "\n",
    "    return field_list\n",
    "\n",
    "\n",
    "def get_departments():\n",
    "    r = requests.get(URL)\n",
    "    departments_table = []\n",
    "    departments_list = []\n",
    "    soup = BeautifulSoup(r.text, \"html.parser\")  # Use 'html.parser' as the parser\n",
    "\n",
    "    # Find the table with the specified id\n",
    "    table = soup.find(\"table\", {\"id\": \"formPrincipal:tabela\"})\n",
    "    if table:\n",
    "        tbody = table.find(\"tbody\")\n",
    "\n",
    "        # Find all <tr> elements within <tbody>\n",
    "        tr_elements = tbody.find_all(\"tr\")\n",
    "\n",
    "        for i, tr in enumerate(tr_elements):\n",
    "            tableCode = tr.find(\n",
    "                \"span\", {\"id\": \"formPrincipal:tabela:{}:codigoDepartamento\".format(i)}\n",
    "            )\n",
    "            tableName = tr.find(\n",
    "                \"span\", {\"id\": \"formPrincipal:tabela:{}:descricao\".format(i)}\n",
    "            )\n",
    "            # if tableCode and tableCode.text.strip() in desired_departments:\n",
    "            #     departments_table.append(\n",
    "            #         {\"id\": tableCode.text.strip(), \"name\": tableName.text.strip()}\n",
    "            #     )\n",
    "            #     departments_list.append(tableCode.text.strip())\n",
    "            departments_table.append(\n",
    "                {\"id\": tableCode.text.strip(), \"name\": tableName.text.strip()}\n",
    "            )\n",
    "            departments_list.append(tableCode.text.strip())\n",
    "\n",
    "        departments_df = pd.DataFrame(departments_table)\n",
    "        departments_df[\"created_at\"] = datetime.now()\n",
    "        return departments_df, departments_list\n",
    "\n",
    "\n",
    "def get_discipline_tables(departments_list):\n",
    "    discipline_dict = {\"id\": [], \"name\": [], \"description\": [], \"department_id\": []}\n",
    "    class_dict = {\n",
    "        \"id\": [],\n",
    "        \"class_number\": [],\n",
    "        \"discipline_id\": [],\n",
    "        \"professor\": [],\n",
    "    }\n",
    "    schedule_dict = {\n",
    "        \"id\": [],\n",
    "        \"discipline_class_id\": [],\n",
    "        \"day_of_week\": [],\n",
    "        \"start_time\": [],\n",
    "        \"end_time\": [],\n",
    "        \"class_type\": [],\n",
    "    }\n",
    "\n",
    "    for department in departments_list:\n",
    "        if department not in desired_departments:\n",
    "            continue\n",
    "\n",
    "        print(f\"Buscando {department}\")\n",
    "        html_content = get_HTML_content(URL, department)\n",
    "        columns_list = [\n",
    "            \"codigo\",\n",
    "            \"disciplina\",\n",
    "            \"descricao\",\n",
    "            \"turma\",\n",
    "            \"horario\",\n",
    "            \"professores\",\n",
    "        ]\n",
    "        columns_dict_list = {}\n",
    "\n",
    "        for column_name in columns_list:\n",
    "            field = get_field_list(html_content, column_name)\n",
    "            columns_dict_list[column_name] = field\n",
    "\n",
    "        for i in range(len(columns_dict_list[\"codigo\"])):\n",
    "            discipline_dict[\"id\"].append(columns_dict_list[\"codigo\"][i])\n",
    "            discipline_dict[\"name\"].append(columns_dict_list[\"disciplina\"][i])\n",
    "            discipline_dict[\"description\"].append(columns_dict_list[\"descricao\"][i])\n",
    "            discipline_dict[\"department_id\"].append(department)\n",
    "\n",
    "            discipline_class_id = str(uuid.uuid4())\n",
    "            class_dict[\"id\"].append(discipline_class_id)\n",
    "            class_dict[\"class_number\"].append(columns_dict_list[\"turma\"][i])\n",
    "            class_dict[\"discipline_id\"].append(columns_dict_list[\"codigo\"][i])\n",
    "            class_dict[\"professor\"].append(columns_dict_list[\"professores\"][i])\n",
    "\n",
    "            schedule_entries = parse_schedule_string(columns_dict_list[\"horario\"][i])\n",
    "            for entry in schedule_entries:\n",
    "                schedule_dict[\"id\"].append(str(uuid.uuid4()))\n",
    "                schedule_dict[\"discipline_class_id\"].append(discipline_class_id)\n",
    "                schedule_dict[\"day_of_week\"].append(entry[\"day_of_week\"])\n",
    "                schedule_dict[\"start_time\"].append(entry[\"start_time\"])\n",
    "                schedule_dict[\"end_time\"].append(entry[\"end_time\"])\n",
    "                schedule_dict[\"class_type\"].append(entry[\"class_type\"])\n",
    "\n",
    "    discipline_df = pd.DataFrame(discipline_dict).drop_duplicates()\n",
    "    class_df = pd.DataFrame(class_dict)\n",
    "    schedule_df = pd.DataFrame(schedule_dict)\n",
    "\n",
    "    currentTime = datetime.now()\n",
    "\n",
    "    discipline_df[\"created_at\"] = currentTime\n",
    "    class_df[\"created_at\"] = currentTime\n",
    "    class_df[\"semester\"] = semester\n",
    "    schedule_df[\"created_at\"] = currentTime\n",
    "\n",
    "    return discipline_df, class_df, schedule_df\n",
    "\n",
    "\n",
    "department_df, departments_list = get_departments()\n",
    "department_df.to_csv(f\"{tables_folder_path}/department.csv\", index=False)\n",
    "department_df.to_sql(\"department\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "(\n",
    "    discipline_df,\n",
    "    discipline_class_df,\n",
    "    discipline_class_schedule_df,\n",
    ") = get_discipline_tables(departments_list)\n",
    "discipline_df.to_csv(f\"{tables_folder_path}/discipline.csv\", index=False)\n",
    "discipline_df.to_sql(\"discipline\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "discipline_class_df.to_csv(f\"{tables_folder_path}/discipline_class.csv\", index=False)\n",
    "discipline_class_df.to_sql(\"discipline_class\", engine, index=False, if_exists=\"replace\")\n",
    "\n",
    "discipline_class_schedule_df.to_csv(\n",
    "    f\"{tables_folder_path}/discipline_class_schedule.csv\", index=False\n",
    ")\n",
    "discipline_class_schedule_df.to_sql(\n",
    "    \"discipline_class_schedule\", engine, index=False, if_exists=\"replace\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando ./courses_pdfs/engenharia-de-computacao.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: jul 24, 2023 5:57:13 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:14 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:14 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:15 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:15 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando ./courses_pdfs/engenharia-de-producao-jm.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: jul 24, 2023 5:57:15 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:16 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:16 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:16 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:16 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando ./courses_pdfs/sistemas-de-informacao.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: jul 24, 2023 5:57:17 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:18 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:18 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Buscando ./courses_pdfs/engenharia-eletrica.pdf\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Got stderr: jul 24, 2023 5:57:18 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:19 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:20 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "jul 24, 2023 5:57:20 PM org.apache.pdfbox.pdmodel.font.PDTrueTypeFont <init>\n",
      "ADVERTÃNCIA: Using fallback font 'LiberationSans-Bold' for 'Arial-Black'\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tabula\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import uuid\n",
    "\n",
    "import re\n",
    "\n",
    "code_pattern = r\"[A-Z]{3}\\d{3}\"\n",
    "subject_pattern = r\"\\b[A-Z]+\\b\"\n",
    "classes_pattern = r\"^(T P|T|P)$\"\n",
    "prerequisite_pattern = r\"[A-Z]{3}\\d{3}|\\d+\\s+horas\"\n",
    "chs_che_pattern = r\"^\\d+\\/\\d+$\"\n",
    "\n",
    "\n",
    "def get_col_idx(df_value, pattern):\n",
    "    indexes = []\n",
    "    for i, item in enumerate(df_value):\n",
    "        if i == 0:\n",
    "            continue\n",
    "        match = re.search(pattern, str(item), re.UNICODE)\n",
    "        if match is not None:\n",
    "            indexes.append(i)\n",
    "\n",
    "    return indexes\n",
    "\n",
    "\n",
    "def replace_carriage_return(arr):\n",
    "    series = pd.Series(arr)\n",
    "\n",
    "    # Replace '\\r' with an empty string in the Series values\n",
    "    series = series.astype(str).str.replace(r\"\\r\", \"\")\n",
    "\n",
    "    return series\n",
    "\n",
    "\n",
    "def get_prerequisites(df_value):\n",
    "    prerequisites = []\n",
    "    prereq_idx = get_col_idx(df_value, prerequisite_pattern)\n",
    "    if len(prereq_idx) > 0:\n",
    "        prerequisites = [df_value[i] for i in prereq_idx]\n",
    "\n",
    "    return format_prerequisites(prerequisites)\n",
    "\n",
    "\n",
    "def get_discipline(df_value):\n",
    "    subject_idx = get_col_idx(df_value, subject_pattern)\n",
    "    if len(subject_idx) > 0:\n",
    "        return re.sub(code_pattern, \"\", df_value[subject_idx[0]])\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_chs_che(df_value):\n",
    "    chs_che_idx = get_col_idx(df_value, chs_che_pattern)\n",
    "\n",
    "    if len(chs_che_idx) > 0:\n",
    "        chs_che_list = df_value[chs_che_idx[0]].split(\"/\")\n",
    "        return tuple(map(int, chs_che_list))\n",
    "\n",
    "    return (\"\", \"\")\n",
    "\n",
    "\n",
    "def get_classes(df_value, classes_idx):\n",
    "    classes = []\n",
    "    if len(classes_idx) > 0:\n",
    "        classes = [df_value[i] for i in classes_idx]\n",
    "        return \" \".join(classes)\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_period(df_value, eletiva):\n",
    "    if eletiva or not df_value[0]:\n",
    "        return \"\"\n",
    "\n",
    "    for item in reversed(df_value):\n",
    "        if item:\n",
    "            return int(item)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_period2(df_value, mandatory):\n",
    "    if not mandatory or not df_value[0]:\n",
    "        return \"\"\n",
    "\n",
    "    for item in reversed(df_value):\n",
    "        if item:\n",
    "            return int(item)\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def get_cha(df_value, chs):\n",
    "    if not chs:\n",
    "        return \"\"\n",
    "\n",
    "    chs_alt = chs * 1.2\n",
    "\n",
    "    if len(df_value) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    for text in df_value:\n",
    "        try:\n",
    "            formatted_text = int(text)\n",
    "            if formatted_text == chs or formatted_text == chs_alt:\n",
    "                return formatted_text\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "    return \"\"\n",
    "\n",
    "\n",
    "def format_prerequisites(df_value):\n",
    "    if len(df_value) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    requisites = []\n",
    "    for text in df_value:\n",
    "        if not text:\n",
    "            continue\n",
    "\n",
    "        matches = re.findall(code_pattern, text)\n",
    "        requisites.extend(matches)\n",
    "\n",
    "    joined_matches = \" \".join(requisites)\n",
    "    return joined_matches\n",
    "\n",
    "\n",
    "def get_formatted_df(df, eletiva):\n",
    "    df = df.replace({r\"\\r\": \" \"}, regex=True)\n",
    "    df_struct = {\n",
    "        \"codigo\": [],\n",
    "        \"disciplina\": [],\n",
    "        \"prerequisitos\": [],\n",
    "        \"chs\": [],\n",
    "        \"che\": [],\n",
    "        \"cha\": [],\n",
    "        \"aulas\": [],\n",
    "        \"periodo\": [],\n",
    "    }\n",
    "    ideal_df = pd.DataFrame(data=df_struct)\n",
    "\n",
    "    ideal_columns = ideal_df.columns.to_list()\n",
    "\n",
    "    column_names = df.columns.tolist()\n",
    "    disc_indexes = [i for i, item in enumerate(column_names) if \"DISCIPLINAS\" in item]\n",
    "\n",
    "    iterIdx = -1\n",
    "    classes_idx = []\n",
    "\n",
    "    for idx, value in enumerate(df.values):\n",
    "        if idx == 0:\n",
    "            iterIdx = -1\n",
    "            classes_idx = get_col_idx(value, classes_pattern)\n",
    "            ideal_df.at[0, ideal_columns[0]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[1]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[2]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[3]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[4]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[5]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[6]] = \"\"\n",
    "            ideal_df.at[0, ideal_columns[7]] = \"\"\n",
    "            continue\n",
    "\n",
    "        chs, che = get_chs_che(value)\n",
    "        ideal_df.at[idx, ideal_columns[0]] = value[0]\n",
    "        ideal_df.at[idx, ideal_columns[1]] = get_discipline(value)\n",
    "        ideal_df.at[idx, ideal_columns[2]] = get_prerequisites(value)\n",
    "        ideal_df.at[idx, ideal_columns[3]] = chs\n",
    "        ideal_df.at[idx, ideal_columns[4]] = che\n",
    "        ideal_df.at[idx, ideal_columns[5]] = get_cha(value, chs)\n",
    "        ideal_df.at[idx, ideal_columns[6]] = get_classes(value, classes_idx)\n",
    "        ideal_df.at[idx, ideal_columns[7]] = get_period(value, eletiva)\n",
    "\n",
    "        if value[0] != \"\":\n",
    "            iterIdx = -1\n",
    "            continue\n",
    "\n",
    "        if iterIdx == -1:\n",
    "            iterIdx = idx - 1\n",
    "\n",
    "        ideal_df.loc[iterIdx, :] = [\n",
    "            f\"{item1} {item2}\".strip()\n",
    "            for item1, item2 in zip(ideal_df.values[iterIdx], ideal_df.values[idx])\n",
    "        ]\n",
    "\n",
    "    ideal_df.drop(ideal_df[ideal_df[\"codigo\"] == \"\"].index, inplace=True)\n",
    "    ideal_df[[\"chs\", \"che\", \"cha\"]] = ideal_df[[\"chs\", \"che\", \"cha\"]].astype(int)\n",
    "    ideal_df[\"eletiva\"] = eletiva\n",
    "\n",
    "    return ideal_df\n",
    "\n",
    "\n",
    "def get_prerequisite_df(discipline_course_df):\n",
    "    prerequisite_df = pd.concat(\n",
    "        discipline_course_df.apply(create_prerequisite_rows, axis=1).tolist(),\n",
    "        ignore_index=True,\n",
    "    )\n",
    "\n",
    "    prerequisite_df[\"created_at\"] = datetime.now()\n",
    "    return prerequisite_df\n",
    "\n",
    "\n",
    "def create_prerequisite_rows(row):\n",
    "    prerequisites = row[\"prerequisites\"].split()\n",
    "    if len(prerequisites):\n",
    "        return pd.DataFrame(\n",
    "            {\n",
    "                \"id\": [str(uuid.uuid4()) for _ in range(len(prerequisites))],\n",
    "                \"discipline_course_id\": [row[\"id\"] for _ in range(len(prerequisites))],\n",
    "                \"prerequisite_discipline_id\": prerequisites,\n",
    "            }\n",
    "        )\n",
    "    else:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "\n",
    "def get_discipline_course_tables(df, course_id, mandatory):\n",
    "    df = df.replace({r\"\\r\": \" \"}, regex=True)\n",
    "    df_struct = {\"id\": [], \"discipline_id\": [], \"period\": [], \"prerequisites\": []}\n",
    "    discipline_course_df = pd.DataFrame(data=df_struct)\n",
    "\n",
    "    ideal_columns = discipline_course_df.columns.to_list()\n",
    "\n",
    "    column_names = df.columns.tolist()\n",
    "    disc_indexes = [i for i, item in enumerate(column_names) if \"DISCIPLINAS\" in item]\n",
    "\n",
    "    iterIdx = -1\n",
    "\n",
    "    for idx, value in enumerate(df.values):\n",
    "        if idx == 0:\n",
    "            iterIdx = -1\n",
    "            discipline_course_df.at[0, \"id\"] = \"\"\n",
    "            discipline_course_df.at[0, \"discipline_id\"] = \"\"\n",
    "            discipline_course_df.at[0, \"period\"] = \"\"\n",
    "            discipline_course_df.at[0, \"prerequisites\"] = \"\"\n",
    "            continue\n",
    "\n",
    "        discipline_course_df.at[idx, \"id\"] = str(uuid.uuid4())\n",
    "        discipline_course_df.at[idx, \"discipline_id\"] = value[0]\n",
    "        discipline_course_df.at[idx, \"period\"] = get_period2(value, mandatory)\n",
    "        discipline_course_df.at[idx, \"prerequisites\"] = get_prerequisites(value)\n",
    "\n",
    "        if value[0] != \"\":\n",
    "            iterIdx = -1\n",
    "            continue\n",
    "\n",
    "        if iterIdx == -1:\n",
    "            iterIdx = idx - 1\n",
    "\n",
    "        discipline_course_df.loc[iterIdx, :] = [\n",
    "            f\"{item1} {item2}\".strip()\n",
    "            for item1, item2 in zip(\n",
    "                discipline_course_df.values[iterIdx], discipline_course_df.values[idx]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    discipline_course_df.drop(\n",
    "        discipline_course_df[discipline_course_df[\"discipline_id\"] == \"\"].index,\n",
    "        inplace=True,\n",
    "    )\n",
    "    discipline_course_df[\"mandatory\"] = mandatory\n",
    "    discipline_course_df[\"course_id\"] = course_id\n",
    "    prerequisite_df = pd.DataFrame()\n",
    "    if not discipline_course_df.empty:\n",
    "        prerequisite_df = get_prerequisite_df(discipline_course_df)\n",
    "\n",
    "    discipline_course_df.drop(\"prerequisites\", axis=1)\n",
    "\n",
    "    return discipline_course_df, prerequisite_df\n",
    "\n",
    "\n",
    "def save_table_to_csv(df):\n",
    "    # Save the DataFrame to a CSV file\n",
    "    filename = \"table.csv\"\n",
    "    df.to_csv(filename, index=False)\n",
    "    print(f\"Table saved as {filename}\")\n",
    "\n",
    "\n",
    "def save_table_to_json(df):\n",
    "    # Save the DataFrame to a CSV file\n",
    "    filename = \"table.json\"\n",
    "    df.to_json(filename, orient=\"records\")\n",
    "    print(f\"Table saved as {filename}\")\n",
    "\n",
    "\n",
    "def scrape_table_from_pdf(pdf_path):\n",
    "    # Read the table from the PDF file\n",
    "    df_list = tabula.read_pdf(pdf_path, pages=\"all\")\n",
    "\n",
    "    course_id = os.path.splitext(os.path.basename(pdf_path))[0]\n",
    "\n",
    "    discipline_course_dfs = []\n",
    "    prerequisite_dfs = []\n",
    "\n",
    "    for df in df_list:\n",
    "        df.fillna(\"\", inplace=True)\n",
    "        # Filter and select the desired columns based on the header\n",
    "        header = df.columns.to_list()\n",
    "        eletivas_column = next(\n",
    "            (col for col in header if \"DISCIPLINAS ELETIVAS\" in col), None\n",
    "        )\n",
    "        obrigatorias_column = next(\n",
    "            (col for col in header if \"DISCIPLINAS OBRIGATÃRIAS\" in col), None\n",
    "        )\n",
    "\n",
    "        if \"DISCIPLINAS OBRIGATÃRIAS\" in header:\n",
    "            discipline_course_df, prerequiste_df = get_discipline_course_tables(\n",
    "                df, course_id, True\n",
    "            )\n",
    "            discipline_course_dfs.append(discipline_course_df)\n",
    "            prerequisite_dfs.append(prerequiste_df)\n",
    "\n",
    "        elif (\n",
    "            \"DISCIPLINAS ELETIVAS\" in header\n",
    "            or \"DISCIPLINAS ELETIVAS PRÃ-REQUISITO\" in header\n",
    "        ):\n",
    "            discipline_course_df, prerequiste_df = get_discipline_course_tables(\n",
    "                df, course_id, False\n",
    "            )\n",
    "            discipline_course_dfs.append(discipline_course_df)\n",
    "            prerequisite_dfs.append(prerequiste_df)\n",
    "        else:\n",
    "            continue\n",
    "\n",
    "    combined_discipline_course_df = pd.concat(discipline_course_dfs, ignore_index=True)\n",
    "\n",
    "    combined_prerequisite_df = pd.concat(prerequisite_dfs, ignore_index=True)\n",
    "    discipline_course_df = combined_discipline_course_df[\n",
    "        combined_discipline_course_df[\"discipline_id\"].isin(discipline_df[\"id\"])\n",
    "    ]\n",
    "\n",
    "    prerequisite_df = combined_prerequisite_df\n",
    "    if not combined_prerequisite_df.empty:\n",
    "        prerequisite_df = combined_prerequisite_df[\n",
    "            combined_prerequisite_df[\"discipline_course_id\"].isin(\n",
    "                discipline_course_df[\"id\"]\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    return discipline_course_df, prerequisite_df\n",
    "\n",
    "\n",
    "files = os.listdir(pdfs_folder_path)\n",
    "pdf_files = [\n",
    "    os.path.join(pdfs_folder_path, file) for file in files if file.endswith(\".pdf\")\n",
    "]\n",
    "\n",
    "for pdf_file in [\n",
    "    \"./courses_pdfs/engenharia-de-computacao.pdf\",\n",
    "    \"./courses_pdfs/engenharia-de-producao-jm.pdf\",\n",
    "    \"./courses_pdfs/sistemas-de-informacao.pdf\",\n",
    "    \"./courses_pdfs/engenharia-eletrica.pdf\",\n",
    "]:\n",
    "    print(f\"Buscando {pdf_file}\")\n",
    "    discipline_course_df, prerequisite_df = scrape_table_from_pdf(pdf_file)\n",
    "    discipline_course_df.to_csv(\n",
    "        f\"{tables_folder_path}/discipline_course.csv\",\n",
    "        mode=\"a\",\n",
    "        header=not os.path.exists(f\"{tables_folder_path}/discipline_course.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    discipline_course_df.to_sql(\n",
    "        \"discipline_course\", con=engine, if_exists=\"append\", index=False\n",
    "    )\n",
    "\n",
    "    prerequisite_df.to_csv(\n",
    "        f\"{tables_folder_path}/prerequisite.csv\",\n",
    "        mode=\"a\",\n",
    "        header=not os.path.exists(f\"{tables_folder_path}/prerequisite.csv\"),\n",
    "        index=False,\n",
    "    )\n",
    "    prerequisite_df.to_sql(\"prerequisite\", con=engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "# discipline_course_df, prerequisite_df = scrape_table_from_pdf(\n",
    "#     \"./courses_pdfs/fisica-bacharelado.pdf\"\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(pdfs_folder_path)\n",
    "pdf_files = [\n",
    "    os.path.join(pdfs_folder_path, file) for file in files if file.endswith(\".pdf\")\n",
    "]\n",
    "print(len(pdf_files))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
